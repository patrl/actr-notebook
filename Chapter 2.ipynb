{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chapter 1\n",
    "\n",
    "## Creating arbitrary chunks\n",
    "\n",
    "First, we conditionally import the `pyactr` module as `actr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pyactr as actr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's first declare a chunk type to model the lexicon. The `chunktype` function creates a type `word` with some slots, seperated by commas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "actr.chunktype(\"word\", \"form, meaning, category, number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now that we've declared a chunk type, we can create chunks of that type.\n",
    "\n",
    "Note that the function `makechunk` only has two obligatory arguments:\n",
    " - `typename`\n",
    " - `nameofchunk`\n",
    " \n",
    " Unspecified slots are left empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word(category= noun, form= car, meaning= [[car]], number= sg)\n"
     ]
    }
   ],
   "source": [
    "carLexeme = actr.makechunk(nameofchunk=\"car1\",\n",
    "                          typename=\"word\",\n",
    "                          form=\"car\",\n",
    "                          meaning=\"[[car]]\",\n",
    "                          category=\"noun\",\n",
    "                          number=\"sg\")\n",
    "\n",
    "print(carLexeme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "N.b. that chunks can be extended with new slots not originally present in the chunk type declaration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nix/store/x26x643i7wrljvcd2c9zkdd89f4mvzxa-python3-3.7.7-env/lib/python3.7/site-packages/pyactr/chunks.py:130: UserWarning: Chunk type word is extended with new attributes\n",
      "  warnings.warn(\"Chunk type %s is extended with new attributes\" % typename)\n"
     ]
    }
   ],
   "source": [
    "carLexeme2 = actr.makechunk(nameofchunk=\"car1\",\n",
    "                          typename=\"word\",\n",
    "                          form=\"car\",\n",
    "                          meaning=\"[[car]]\",\n",
    "                          category=\"noun\",\n",
    "                          number=\"sg\",\n",
    "                           synfunction=\"subj\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The `chunkstring` function is an alternative method for declaring a new chunk:\n",
    "\n",
    "(n.b. triple quotation in python indicates that a string can appear on more than one line.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word(category= noun, form= car, meaning= [[car]], number= sg, synfunction= subj)\n"
     ]
    }
   ],
   "source": [
    "carLexeme3 = actr.chunkstring(string=\"\"\"\n",
    "    isa word\n",
    "    form car\n",
    "    meaning '[[car]]'\n",
    "    category noun\n",
    "    number sg\n",
    "    synfunction subj\n",
    "\"\"\")\n",
    "\n",
    "print(carLexeme3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Treating chunks as attribute value sets induces natural notions of equality and implication. The standard python comparison operators are overloaded to reflect this.\n",
    "\n",
    "N.b. the chunk type is completely ignored for the purpose of these comparisons; the chunk type is just syntactic sugar for human modellers, it has no status in the theory of ACT-R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carLexeme2 == carLexeme3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carLexeme == carLexeme2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carLexeme < carLexeme2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beyond arbitrary chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We're going to build an extremely simple mind that can do subject agreement. First, let's build a container for a mind (a model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "agreement = actr.ACTRModel ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For convenience, any model comes with a declarative memory module with an empty retrieval buffer. Modules are attributes of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agreement.decmem\n",
    "agreement.goal\n",
    "agreement.retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's declare a shorter alias for the declarative memory module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dm = agreement.decmem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can invoke the `add` method associated with the declarative memory module in order to add chunks to declarative memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{word(category= noun, form= car, meaning= [[car]], number= sg, synfunction= subj): array([0.])}\n"
     ]
    }
   ],
   "source": [
    "dm.add(carLexeme2)\n",
    "print(dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that when we inspect `dm`, we see the chunk we just added, alongside the chunk encoding time. Since we haven't run the model yet, this time is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In order to model subject-verb agreement, we're going to be making some simplifying assumptions. We won't say anything about how parsing works, just assume that declarative memory already contains the suvject  of the clause, and the current verb is present in the goal buffer.\n",
    "\n",
    "We're going to need three productions in procedural memory:\n",
    "- if the goal has a verb chunk, and the current task is to agree, then retrieve the subject.\n",
    "- If the number of the subject is `=x`, then the number of the verb in the goal should also be `=x`.\n",
    "- If the verb is assigned a number, the task is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Noun retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Productions are created by the method `productionstring`.\n",
    "\n",
    "The `==>` acts as a seperator between the preconditions and the actions.\n",
    "\n",
    "Preconditions are specified for two buffers.\n",
    "- `=g>` indicates the target buffer (`g`: the *goal* buffer), and the type of precondition this buffer has to satisfy (`=`: subsumption; this is highly confusing). So, the chunk currently stored in the goal buffer must be subsumed by the chunk characterised on the following lines. Since the precondition is stated as subsumption, the chunk in the goal buffer could have other slot value pairs (how do we square this with subsumption?).\n",
    "- `?retrieval>` checks whether the retrieval buffer is in a certain state. The `?` symbol indicates that we're interested in the state of the buffer, not the chunk inside of it. The buffer must be empty for this precondition to be satisfied.\n",
    "\n",
    "In general we can use `?` to submit a variety of queries regarding the state of buffers.\n",
    "\n",
    "If the preconditions are both met, two actions are triggered.\n",
    "-  The current task of the `goal_lexeme` chunk is changed to `trigger_agreement`. This isn' reflected in the book, but it looks like this is really the only thing that needs to be specified.\n",
    "- The second action is to add a new chunk to the retrieval buffer, this is what the `+` symbol indicates. What gets added to the retrieval buffer is a noun that is the subject of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'=g': goal_lexeme(category= verb, task= agree), '?retrieval': {'buffer': 'empty'}}\n",
       "==>\n",
       "{'=g': goal_lexeme(category= verb, task= trigger_agreement), '+retrieval': word(category= noun, form= , meaning= , number= , synfunction= subject)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actr.chunktype(\"goal_lexeme\",\"category, task\")\n",
    "\n",
    "agreement.productionstring(name=\"retrieve\", string=\"\"\"\n",
    "    =g>\n",
    "    isa goal_lexeme\n",
    "    category verb\n",
    "    task agree\n",
    "    ?retrieval>\n",
    "    buffer empty\n",
    "    ==>\n",
    "    =g>\n",
    "    isa goal_lexeme\n",
    "    task trigger_agreement\n",
    "    category verb\n",
    "    +retrieval>\n",
    "    isa word\n",
    "    category noun\n",
    "    synfunction subject\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agreement\n",
    "\n",
    "We now define a second production rule to actually perform the agreement. This comes with two preconditions that check that we are in the correct state:\n",
    "- The chunk in the goal buffer is subsumed by the chunk described here.\n",
    "- The chunk in the retrieval buffer is subsumed by the chunk described here.\n",
    "\n",
    "If we're in the correct state, the action is triggered:\n",
    "- the chunk in the goal buffer is *updated* such that a new number specification is added, which matches the one on the subject noun we retrieved from declarative memory.\n",
    "- The task is marked as done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'=g': goal_lexeme(category= verb, number= , task= trigger_agreement), '=retrieval': word(category= noun, form= , meaning= , number= =x, synfunction= subject)}\n",
       "==>\n",
       "{'=g': goal_lexeme(category= verb, number= =x, task= done)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agreement.productionstring(name=\"agree\", string=\"\"\"\n",
    "    =g>\n",
    "    isa goal_lexeme\n",
    "    task trigger_agreement\n",
    "    category verb\n",
    "    =retrieval>\n",
    "    isa word\n",
    "    category noun\n",
    "    synfunction subject\n",
    "    number =x\n",
    "    ==>\n",
    "    =g>\n",
    "    isa goal_lexeme\n",
    "    category verb\n",
    "    number =x\n",
    "    task done\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Clean-up\n",
    "\n",
    "The final production rule mops things up, flushing the goal buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'=g': goal_lexeme(category= , number= , task= done)}\n",
       "==>\n",
       "{'~g': None}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agreement.productionstring(name=\"done\", string=\"\"\"\n",
    "=g>\n",
    "isa goal_lexeme\n",
    "task done\n",
    "==>\n",
    "~g>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Running the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To get things off the ground, we need to add a goal lexeme to the goal buffer. Since according to ACT-R, higher cognition is goal driven, nothing will happen in the absence of a goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{goal_lexeme(category= verb, number= , task= agree)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actr.chunktype(\"goal_lexeme\", \"task, category, number\")\n",
    "\n",
    "agreement.goal.add(actr.chunkstring(string=\"\"\"\n",
    "    isa goal_lexeme\n",
    "    task agree\n",
    "    category verb\"\"\"))\n",
    "\n",
    "agreement.goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "agreement_sim = agreement.simulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- TODO the following currently doesn't deliver the expected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0, 'PROCEDURAL', 'RULE SELECTED: retrieve')\n",
      "(0.05, 'PROCEDURAL', 'RULE FIRED: retrieve')\n",
      "(0.05, 'g', 'MODIFIED')\n",
      "(0.05, 'retrieval', 'START RETRIEVAL')\n",
      "(0.05, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0.05, 'PROCEDURAL', 'NO RULE FOUND')\n",
      "(0.1, 'retrieval', 'RETRIEVED: None')\n",
      "(0.1, 'PROCEDURAL', 'CONFLICT RESOLUTION')\n",
      "(0.1, 'PROCEDURAL', 'NO RULE FOUND')\n"
     ]
    }
   ],
   "source": [
    "agreement_sim.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Observe that when we run a simulation, every cognitive step takes 50ms. This is the ACT-R default for an elementary cognitive step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# More examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The counting model\n",
    "\n",
    "### The task\n",
    "\n",
    "- Given an initial number $n$, and a final number $m$, increment $n$ until $m$ is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
